{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4be5d807",
   "metadata": {},
   "source": [
    "# Quickstart: Agentic retrieval in Azure AI Search\n",
    "\n",
    "Use this notebook to get started with [agentic retrieval](https://learn.microsoft.com/azure/search/search-agentic-retrieval-concept) in Azure AI Search, which integrates conversation history and large language models (LLMs) on Azure OpenAI to plan, retrieve, and synthesize complex queries.\n",
    "\n",
    "Steps in this notebook include:\n",
    "\n",
    "+ Creating an `earth_at_night` search index.\n",
    "\n",
    "+ Loading the index with documents from a GitHub URL.\n",
    "\n",
    "+ Creating an `earth-search-agent` in Azure AI Search that points to an LLM for query planning.\n",
    "\n",
    "+ Using the agent to fetch and rank relevant information from the index.\n",
    "\n",
    "+ Generating answers using the Azure OpenAI client.\n",
    "\n",
    "This notebook provides a high-level demonstration of agentic retrieval. For more detailed guidance, see [Quickstart: Run agentic retrieval in Azure AI Search](https://learn.microsoft.com/azure/search/search-get-started-agentic-retrieval)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712b97d",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "+ An [Azure AI Search service](https://learn.microsoft.com/azure/search/search-create-service-portal) on the Basic tier or higher with [semantic ranker enabled](https://learn.microsoft.com/azure/search/semantic-how-to-enable-disable).\n",
    "\n",
    "+ An [Azure OpenAI resource](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource).\n",
    "\n",
    "+ A [supported model](https://learn.microsoft.com/azure/search/search-agentic-retrieval-how-to-create#supported-models) deployed to your Azure OpenAI resource. This notebook uses `gpt-4.1-mini`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5fbd46",
   "metadata": {},
   "source": [
    "## Configure access\n",
    "\n",
    "This notebook assumes authentication and authorization using Microsoft Entra ID and role assignments. It also assumes that you run the code from your local device.\n",
    "\n",
    "To configure role-based access:\n",
    "\n",
    "1. Sign in to the [Azure portal](https://portal.azure.com).\n",
    "\n",
    "1. [Enable role-based access](https://learn.microsoft.com/azure/search/search-security-enable-roles) on your Azure AI Search service.\n",
    "\n",
    "1. [Create a system-assigned managed identity](https://learn.microsoft.com/azure/search/search-howto-managed-identities-data-sources#create-a-system-managed-identity) on your Azure AI Search service.\n",
    "\n",
    "1. On your Azure AI Search service, [assign the following roles](https://learn.microsoft.com/azure/search/search-security-rbac#how-to-assign-roles-in-the-azure-portal) to yourself.\n",
    "\n",
    "   + **Search Service Contributor**\n",
    "\n",
    "   + **Search Index Data Contributor**\n",
    "\n",
    "   + **Search Index Data Reader**\n",
    "\n",
    "1. On your Azure OpenAI resource, assign **Cognitive Services User** to the managed identity of your search service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733bf308",
   "metadata": {},
   "source": [
    "## Set up connections\n",
    "\n",
    "The `sample.env` file contains environment variables for connections to Azure AI Search and Azure OpenAI. Agentic retrieval requires these connections for document retrieval, query planning, query execution, and answer generation.\n",
    "\n",
    "To set up connections:\n",
    "\n",
    "1. Sign in to the [Azure portal](https://portal.azure.com).\n",
    "\n",
    "2. Retrieve the endpoints for both Azure AI Search and Azure OpenAI.\n",
    "\n",
    "3. Save the `sample.env` file as `.env` on your local device.\n",
    "\n",
    "4. Update the `.env` file with the retrieved endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a54a0f",
   "metadata": {},
   "source": [
    "## Create a virtual environment\n",
    "\n",
    "The `requirements.txt` file contains the dependencies for this notebook. You can install these dependencies in isolation using a virtual environment.\n",
    "\n",
    "To create a virtual environment:\n",
    "\n",
    "1. In Visual Studio Code, open the folder that contains `quickstart.ipynb`.\n",
    "\n",
    "1. Press **Ctrl**+**Shift**+**P** to open the command palette.\n",
    "\n",
    "1. Search for **Python: Create Environment**, and then select **Venv**.\n",
    "\n",
    "1. Select a Python installation. We tested this notebook on Python 3.13.\n",
    "\n",
    "1. Select `requirements.txt` for the dependencies.\n",
    "\n",
    "Creating the virtual environment can take several minutes. When the environment is ready, proceed to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0714a968",
   "metadata": {},
   "source": [
    "## Install packages and load connections\n",
    "\n",
    "This step installs the packages for this notebook and establishes connections to Azure AI Search and Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "041e5d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2df3a118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://kor-ai-search-b.search.windows.net\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True) # Take environment variables from .env.\n",
    "\n",
    "# The following variables from your .env file are used in this notebook\n",
    "answer_model = os.getenv(\"ANSWER_MODEL\", \"gpt-4o\")\n",
    "endpoint = os.environ[\"AZURE_SEARCH_ENDPOINT\"]\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://search.azure.com/.default\")\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX\", \"earth_at_night\")\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "azure_openai_gpt_deployment = os.getenv(\"AZURE_OPENAI_GPT_DEPLOYMENT\", \"gpt-4o\")\n",
    "azure_openai_gpt_model = os.getenv(\"AZURE_OPENAI_GPT_MODEL\", \"gpt-4o\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2025-03-01-preview\")\n",
    "azure_openai_embedding_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-3-large\")\n",
    "azure_openai_embedding_model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-large\")\n",
    "agent_name = os.getenv(\"AZURE_SEARCH_AGENT_NAME\", \"earth-search-agent\")\n",
    "api_version = \"2025-05-01-Preview\"\n",
    "\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e8a088",
   "metadata": {},
   "source": [
    "## Create an index in Azure AI Search\n",
    "\n",
    "This step creates a search index that contains plain text and vector content. You can use an existing index, but it must meet the criteria for [agentic retrieval workloads](https://learn.microsoft.com/azure/search/search-agentic-retrieval-how-to-index). The primary schema requirement is a semantic configuration with a `default_configuration_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee48bec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'earth_at_night' created or updated successfully\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import SearchIndex, SearchField, VectorSearch, VectorSearchProfile, HnswAlgorithmConfiguration, AzureOpenAIVectorizer, AzureOpenAIVectorizerParameters, SemanticSearch, SemanticConfiguration, SemanticPrioritizedFields, SemanticField\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "\n",
    "index = SearchIndex(\n",
    "    name=index_name,\n",
    "    fields=[\n",
    "        SearchField(name=\"id\", type=\"Edm.String\", key=True, filterable=True, sortable=True, facetable=True),\n",
    "        SearchField(name=\"page_chunk\", type=\"Edm.String\", filterable=False, sortable=False, facetable=False),\n",
    "        SearchField(name=\"page_embedding_text_3_large\", type=\"Collection(Edm.Single)\", stored=False, vector_search_dimensions=3072, vector_search_profile_name=\"hnsw_text_3_large\"),\n",
    "        SearchField(name=\"page_number\", type=\"Edm.Int32\", filterable=True, sortable=True, facetable=True)\n",
    "    ],\n",
    "    vector_search=VectorSearch(\n",
    "        profiles=[VectorSearchProfile(name=\"hnsw_text_3_large\", algorithm_configuration_name=\"alg\", vectorizer_name=\"azure_openai_text_3_large\")],\n",
    "        algorithms=[HnswAlgorithmConfiguration(name=\"alg\")],\n",
    "        vectorizers=[\n",
    "            AzureOpenAIVectorizer(\n",
    "                vectorizer_name=\"azure_openai_text_3_large\",\n",
    "                parameters=AzureOpenAIVectorizerParameters(\n",
    "                    resource_url=azure_openai_endpoint,\n",
    "                    deployment_name=azure_openai_embedding_deployment,\n",
    "                    model_name=azure_openai_embedding_model\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    semantic_search=SemanticSearch(\n",
    "        default_configuration_name=\"semantic_config\",\n",
    "        configurations=[\n",
    "            SemanticConfiguration(\n",
    "                name=\"semantic_config\",\n",
    "                prioritized_fields=SemanticPrioritizedFields(\n",
    "                    content_fields=[\n",
    "                        SemanticField(field_name=\"page_chunk\")\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "index_client = SearchIndexClient(endpoint=endpoint, credential=credential)\n",
    "index_client.create_or_update_index(index)\n",
    "print(f\"Index '{index_name}' created or updated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39874f61",
   "metadata": {},
   "source": [
    "## Upload sample documents\n",
    "\n",
    "This notebook uses data from NASA's Earth at Night e-book. The data is retrieved from the [azure-search-sample-data](https://github.com/Azure-Samples/azure-search-sample-data) repository on GitHub and passed to the search client for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ded5147b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents uploaded to index 'earth_at_night'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Azure-Samples/azure-search-sample-data/refs/heads/main/nasa-e-book/earth-at-night-json/documents.json\"\n",
    "documents = requests.get(url).json()\n",
    "\n",
    "with SearchIndexingBufferedSender(endpoint=endpoint, index_name=index_name, credential=credential) as client:\n",
    "    client.upload_documents(documents=documents)\n",
    "\n",
    "print(f\"Documents uploaded to index '{index_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5e0a34",
   "metadata": {},
   "source": [
    "## Create an agent in Azure AI Search\n",
    "\n",
    "This step creates a knowledge agent, which acts as a wrapper for the LLM you deployed to Azure OpenAI. The LLM is used to send queries to an agentic retrieval pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3fe4183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge agent 'earth-search-agent' created or updated successfully\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes.models import KnowledgeAgent, KnowledgeAgentAzureOpenAIModel, KnowledgeAgentTargetIndex, KnowledgeAgentRequestLimits, AzureOpenAIVectorizerParameters\n",
    "\n",
    "agent = KnowledgeAgent(\n",
    "    name=agent_name,\n",
    "    models=[\n",
    "        KnowledgeAgentAzureOpenAIModel(\n",
    "            azure_open_ai_parameters=AzureOpenAIVectorizerParameters(\n",
    "                resource_url=azure_openai_endpoint,\n",
    "                deployment_name=azure_openai_gpt_deployment,\n",
    "                model_name=azure_openai_gpt_model\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    target_indexes=[\n",
    "        KnowledgeAgentTargetIndex(\n",
    "            index_name=index_name,\n",
    "            default_reranker_threshold=2.5\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "index_client.create_or_update_agent(agent)\n",
    "print(f\"Knowledge agent '{agent_name}' created or updated successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1610cefd",
   "metadata": {},
   "source": [
    "## Set up messages\n",
    "\n",
    "Messages are the input for the retrieval route and contain the conversation history. Each message includes a `role` that indicates its origin, such as `assistant` or `user`, and `content` in natural language. The LLM you use determines which roles are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ab7b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "A Q&A agent that can answer questions about the Earth at night.\n",
    "Sources have a JSON format with a ref_id that must be cited in the answer.\n",
    "If you do not have the answer, respond with \"I don't know\".\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": instructions\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4090707f",
   "metadata": {},
   "source": [
    "## Use agentic retrieval to fetch results\n",
    "\n",
    "This step runs the retrieval pipeline to extract relevant information from your search index. Based on the messages and parameters on the retrieval request, the LLM:\n",
    "\n",
    "1. Analyzes the entire conversation history to determine the underlying information need.\n",
    "\n",
    "1. Breaks down the compound user query into focused subqueries.\n",
    " \n",
    "1. Runs each subquery simultaneously against text fields and vector embeddings in your index.\n",
    "\n",
    "1. Uses semantic ranker to rerank the results of all subqueries.\n",
    "\n",
    "1. Merges the results into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "918ded26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.agent import KnowledgeAgentRetrievalClient\n",
    "from azure.search.documents.agent.models import KnowledgeAgentRetrievalRequest, KnowledgeAgentMessage, KnowledgeAgentMessageTextContent, KnowledgeAgentIndexParams\n",
    "\n",
    "agent_client = KnowledgeAgentRetrievalClient(endpoint=endpoint, agent_name=agent_name, credential=credential)\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"\"\"\n",
    "    Why do suburban belts display larger December brightening than urban cores even though absolute light levels are higher downtown?\n",
    "    Why is the Phoenix nighttime street grid is so sharply visible from space, whereas large stretches of the interstate between midwestern cities remain comparatively dim?\n",
    "    \"\"\"\n",
    "})\n",
    "\n",
    "retrieval_result = agent_client.retrieve(\n",
    "    retrieval_request=KnowledgeAgentRetrievalRequest(\n",
    "        messages=[KnowledgeAgentMessage(role=msg[\"role\"], content=[KnowledgeAgentMessageTextContent(text=msg[\"content\"])]) for msg in messages if msg[\"role\"] != \"system\"],\n",
    "        target_index_params=[KnowledgeAgentIndexParams(index_name=index_name, reranker_threshold=2.5)]\n",
    "    )\n",
    ")\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": retrieval_result.response[0].content[0].text\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87a035ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.agent import KnowledgeAgentRetrievalClient\n",
    "from azure.search.documents.agent.models import KnowledgeAgentRetrievalRequest, KnowledgeAgentMessage, KnowledgeAgentMessageTextContent, KnowledgeAgentIndexParams\n",
    "\n",
    "agent_client = KnowledgeAgentRetrievalClient(endpoint=endpoint, agent_name=agent_name, credential=credential)\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"\"\"\n",
    "    Why do suburban belts display larger December brightening than urban cores even though absolute light levels are higher downtown?\n",
    "    Why is the Phoenix nighttime street grid is so sharply visible from space, whereas large stretches of the interstate between midwestern cities remain comparatively dim?\n",
    "    \"\"\"\n",
    "})\n",
    "\n",
    "retrieval_result = agent_client.retrieve(\n",
    "    retrieval_request=KnowledgeAgentRetrievalRequest(\n",
    "        messages=[KnowledgeAgentMessage(role=msg[\"role\"], content=[KnowledgeAgentMessageTextContent(text=msg[\"content\"])]) for msg in messages if msg[\"role\"] != \"system\"],\n",
    "        target_index_params=[KnowledgeAgentIndexParams(index_name=index_name, reranker_threshold=2.5)]\n",
    "    )\n",
    ")\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": retrieval_result.response[0].content[0].text\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18c0e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.agent import KnowledgeAgentRetrievalClient\n",
    "from azure.search.documents.agent.models import KnowledgeAgentRetrievalRequest, KnowledgeAgentMessage, KnowledgeAgentMessageTextContent, KnowledgeAgentIndexParams\n",
    "\n",
    "agent_client = KnowledgeAgentRetrievalClient(endpoint=endpoint, agent_name=agent_name, credential=credential)\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"\"\"\n",
    "    Why do suburban belts display larger December brightening than urban cores even though absolute light levels are higher downtown?\n",
    "    Why is the Phoenix nighttime street grid is so sharply visible from space, whereas large stretches of the interstate between midwestern cities remain comparatively dim?\n",
    "    \"\"\"\n",
    "})\n",
    "\n",
    "retrieval_result = agent_client.retrieve(\n",
    "    retrieval_request=KnowledgeAgentRetrievalRequest(\n",
    "        messages=[KnowledgeAgentMessage(role=msg[\"role\"], content=[KnowledgeAgentMessageTextContent(text=msg[\"content\"])]) for msg in messages if msg[\"role\"] != \"system\"],\n",
    "        target_index_params=[KnowledgeAgentIndexParams(index_name=index_name, reranker_threshold=2.5)]\n",
    "    )\n",
    ")\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": retrieval_result.response[0].content[0].text\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886fc687",
   "metadata": {},
   "source": [
    "### Review the retrieval response, activity, and results\n",
    "\n",
    "Each retrieval response from Azure AI Search includes:\n",
    "\n",
    "+ A unified string that represents grounding data from the search results.\n",
    "\n",
    "+ The query plan.\n",
    "\n",
    "+ Reference data that shows which chunks of the source documents contributed to the unified string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4d78fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "print(\"Response\")\n",
    "print(textwrap.fill(retrieval_result.response[0].content[0].text, width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fccf4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity\n",
      "[\n",
      "  {\n",
      "    \"id\": 0,\n",
      "    \"type\": \"ModelQueryPlanning\",\n",
      "    \"input_tokens\": 2715,\n",
      "    \"output_tokens\": 582\n",
      "  },\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"type\": \"AzureSearchQuery\",\n",
      "    \"target_index\": \"earth_at_night\",\n",
      "    \"query\": {\n",
      "      \"search\": \"Why do suburban belts display larger December brightening than urban cores even though absolute light levels are higher downtown?\"\n",
      "    },\n",
      "    \"query_time\": \"2025-06-28T10:16:29.430Z\",\n",
      "    \"count\": 0,\n",
      "    \"elapsed_ms\": 337\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"type\": \"AzureSearchQuery\",\n",
      "    \"target_index\": \"earth_at_night\",\n",
      "    \"query\": {\n",
      "      \"search\": \"Why is the Phoenix nighttime street grid so sharply visible from space, whereas large stretches of the interstate between midwestern cities remain comparatively dim?\"\n",
      "    },\n",
      "    \"query_time\": \"2025-06-28T10:16:29.816Z\",\n",
      "    \"count\": 0,\n",
      "    \"elapsed_ms\": 375\n",
      "  },\n",
      "  {\n",
      "    \"id\": 3,\n",
      "    \"type\": \"AzureSearchSemanticRanker\",\n",
      "    \"input_tokens\": 52794\n",
      "  }\n",
      "]\n",
      "Results\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(\"Activity\")\n",
    "print(json.dumps([a.as_dict() for a in retrieval_result.activity], indent=2))\n",
    "print(\"Results\")\n",
    "print(json.dumps([r.as_dict() for r in retrieval_result.references], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75386ed1",
   "metadata": {},
   "source": [
    "## Create the Azure OpenAI client\n",
    "\n",
    "So far, this notebook has used agentic retrieval for answer *extraction*, which you can extend to answer *generation* by using the Azure OpenAI client. This enables more detailed, context-rich responses that aren't strictly tied to indexed content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da260539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from azure.identity import get_bearer_token_provider\n",
    "\n",
    "azure_openai_token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    azure_ad_token_provider=azure_openai_token_provider,\n",
    "    api_version=azure_openai_api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56231e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_model = os.getenv(\"OPENAI_MODEL\", \"gpt-4o\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")  # or \"gpt-4o-mini\"\n",
    "\n",
    "# Create OpenAI client\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200bc2a",
   "metadata": {},
   "source": [
    "### Use the Responses API to generate an answer\n",
    "\n",
    "One option for answer generation is the Responses API, which passes the conversation history to the LLM for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2dea5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The larger December brightening in suburban belts compared to urban cores, despite higher absolute\n",
      "light levels downtown, could be due to increased holiday lighting and decorations, which are more\n",
      "prevalent in residential suburbs. Urban cores may have more constant lighting levels year-round,\n",
      "with less seasonal variation.  The Phoenix nighttime street grid's sharp visibility is due to its\n",
      "structured grid layout and widespread street lighting, which highlights the extensive urban area. In\n",
      "contrast, midwestern interstates between cities are less densely populated and have less lighting,\n",
      "contributing to their dimmer appearance from space.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=answer_model,\n",
    "    input=messages\n",
    ")\n",
    "\n",
    "wrapped = textwrap.fill(response.output_text, width=100)\n",
    "print(wrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ffae000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suburban belts often display larger December brightening compared to urban cores due to seasonal\n",
      "lighting, such as holiday decorations, which are more prevalent in suburban residential areas. These\n",
      "temporary increases in lighting can create noticeable brightening despite the urban core having\n",
      "higher absolute light levels year-round.  The Phoenix nighttime street grid is sharply visible from\n",
      "space because it follows a regular, illuminated grid pattern typical of many western U.S. cities.\n",
      "The widespread and evenly distributed street lighting in these areas contributes to the visibility.\n",
      "In contrast, interstates between midwestern cities are less illuminated as they pass through rural\n",
      "areas, with lighting often limited to junctions, service areas, and urban regions, making them\n",
      "appear dimmer from space.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=answer_model,\n",
    "    input=messages\n",
    ")\n",
    "\n",
    "wrapped = textwrap.fill(response.output_text, width=100)\n",
    "print(wrapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c63f0",
   "metadata": {},
   "source": [
    "### Use the Chat Completions API to generate an answer\n",
    "\n",
    "Alternatively, you can use the Chat Completions API for answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9126b4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=answer_model,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "wrapped = textwrap.fill(response.choices[0].message.content, width=100)\n",
    "print(wrapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96436b",
   "metadata": {},
   "source": [
    "## Continue the conversation\n",
    "\n",
    "This step continues the conversation with the knowledge agent, building upon the previous messages and queries to retrieve relevant information from your search index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7034e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"How do I find lava at night?\"\n",
    "})\n",
    "\n",
    "retrieval_result = agent_client.retrieve(\n",
    "    retrieval_request=KnowledgeAgentRetrievalRequest(\n",
    "        messages=[KnowledgeAgentMessage(role=msg[\"role\"], content=[KnowledgeAgentMessageTextContent(text=msg[\"content\"])]) for msg in messages if msg[\"role\"] != \"system\"],\n",
    "        target_index_params=[KnowledgeAgentIndexParams(index_name=index_name, reranker_threshold=2.5)]\n",
    "    )\n",
    ")\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": retrieval_result.response[0].content[0].text\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1cba0c",
   "metadata": {},
   "source": [
    "### Review the retrieval response, activity, and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35a1bfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response\n",
      "[{\"ref_id\":0,\"content\":\"<!-- PageHeader=\\\"Volcanoes\\\" -->\\n\\n## Volcanoes\\n\\n### The Infrared Glows of Kilauea's Lava\n",
      "Flows—Hawaii\\n\\nIn early May 2018, an eruption on Hawaii's Kilauea volcano began to unfold. The eruption took a\n",
      "dangerous turn on May 3, 2018, when new fissures opened in the residential neighborhood of Leilani Estates. During the\n",
      "summer-long eruptive event, other fissures emerged along the East Rift Zone. Lava from vents along the rift zone flowed\n",
      "downslope, reaching the ocean in several areas, and filling in Kapoho Bay.\\n\\nA time series of Landsat 8 imagery shows\n",
      "the progression of the lava flows from May 16 to August 13. The night view combines thermal, shortwave infrared, and\n",
      "near-infrared wavelengths to tease out the very hot lava (bright white), cooling lava (red), and lava flows obstructed\n",
      "by clouds (purple).\\n\\n#### Figure: Location of Kilauea Volcano, Hawaii\\n\\nA globe is shown centered on North America,\n",
      "with a marker placed in the Pacific Ocean indicating the location of Hawaii, to the southwest of the mainland United\n",
      "States.\\n\\n<!-- PageFooter=\\\"Earth at Night\\\" -->\\n<!-- PageNumber=\\\"44\\\" -->\"},{\"ref_id\":1,\"content\":\"For the first\n",
      "time in perhaps a decade, Mount Etna experienced a \\\"flank eruption\\\"—erupting from its side instead of its summit—on\n",
      "December 24, 2018. The activity was accompanied by 130 earthquakes occurring over three hours that morning. Mount Etna,\n",
      "Europe’s most active volcano, has seen periodic activity on this part of the mountain since 2013. The Operational Land\n",
      "Imager (OLI) on the Landsat 8 satellite acquired the main image of Mount Etna on December 28, 2018.\\n\\nThe inset image\n",
      "highlights the active vent and thermal infrared signature from lava flows, which can be seen near the newly formed\n",
      "fissure on the southeastern side of the volcano. The inset was created with data from OLI and the Thermal Infrared\n",
      "Sensor (TIRS) on Landsat 8. Ash spewing from the fissure cloaked adjacent villages and delayed aircraft from landing at\n",
      "the nearby Catania airport. Earthquakes occurred in the subsequent days after the initial eruption and displaced\n",
      "hundreds of people from their homes.\\n\\nFor nighttime images of Mount Etna’s March 2017 eruption, see pages\n",
      "48–51.\\n\\n---\\n\\n### Hazards of Volcanic Ash Plumes and Satellite Observation\\n\\nWith the help of moonlight, satellite\n",
      "instruments can track volcanic ash plumes, which present significant hazards to airplanes in flight. The volcanic\n",
      "ash—composed of tiny pieces of glass and rock—is abrasive to engine turbine blades, and can melt on the blades and other\n",
      "engine parts, causing damage and even engine stalls. This poses a danger to both the plane’s integrity and passenger\n",
      "safety. Volcanic ash also reduces visibility for pilots and can cause etching of windshields, further reducing pilots’\n",
      "ability to see. Nightlight images can be combined with thermal images to provide a more complete view of volcanic\n",
      "activity on Earth’s surface.\\n\\nThe VIIRS Day/Night Band (DNB) on polar-orbiting satellites uses faint light sources\n",
      "such as moonlight, airglow (the atmosphere’s self-illumination through chemical reactions), zodiacal light (sunlight\n",
      "scattered by interplanetary dust), and starlight from the Milky Way. Using these dim light sources, the DNB can detect\n",
      "changes in clouds, snow cover, and sea ice:\\n\\n#### Table: Light Sources Used by VIIRS DNB\\n\\n| Light Source         |\n",
      "Description\n",
      "|\\n|----------------------|------------------------------------------------------------------------------|\\n| Moonlight\n",
      "| Reflected sunlight from the Moon, illuminating Earth's surface at night      |\\n| Airglow              | Atmospheric\n",
      "self-illumination from chemical reactions                        |\\n| Zodiacal Light       | Sunlight scattered by\n",
      "interplanetary dust                                    |\\n| Starlight/Milky Way  | Faint illumination provided by stars\n",
      "in the Milky Way                        |\\n\\nGeostationary Operational Environmental Satellites (GOES), managed by NOAA,\n",
      "orbit over Earth’s equator and offer uninterrupted observations of North America. High-latitude areas such as Alaska\n",
      "benefit from polar-orbiting satellites like Suomi NPP, which provide overlapping coverage at the poles, enabling more\n",
      "data collection in these regions. During polar darkness (winter months), VIIRS DNB data allow scientists to:\\n\\n-\n",
      "Observe sea ice formation\\n- Monitor snow cover extent at the highest latitudes\\n- Detect open water for ship\n",
      "navigation\\n\\n#### Table: Satellite Coverage Overview\\n\\n| Satellite Type          | Orbit           | Coverage Area\n",
      "| Special Utility\n",
      "|\\n|------------------------|-----------------|----------------------|----------------------------------------------|\\n|\n",
      "GOES                   | Geostationary   | Equatorial/North America | Continuous regional monitoring              |\\n|\n",
      "Polar-Orbiting (e.g., Suomi NPP) | Polar-orbiting    | Poles/high latitudes      | Overlapping passes; useful during\n",
      "polar night|\\n\\n---\\n\\n### Weather Forecasting and Nightlight Data\\n\\nThe use of nightlight data by weather forecasters\n",
      "is growing as the VIIRS instrument enables observation of clouds at night illuminated by sources such as moonlight and\n",
      "lightning. Scientists use these data to study the nighttime behavior of weather systems, including severe storms, which\n",
      "can develop and strike populous areas at night as well as during the day. Combined with thermal data, visible nightlight\n",
      "data allow the detection of clouds at various heights in the atmosphere, such as dense marine fog. This capability\n",
      "enables weather forecasters to issue marine advisories with higher confidence, leading to greater utility. (See \\\"Marine\n",
      "Layer Clouds—California\\\" on page 56.)\\n\\nIn this section of the book, you will see how nightlight data are used to\n",
      "observe nature’s spectacular light shows across a wide range of sources.\\n\\n---\\n\\n#### Notable Data from Mount Etna\n",
      "Flank Eruption (December 2018)\\n\\n| Event/Observation                  | Details\n",
      "|\\n|-------------------------------------|----------------------------------------------------------------------------\n",
      "|\\n| Date of Flank Eruption              | December 24, 2018\n",
      "|\\n| Number of Earthquakes               | 130 earthquakes within 3 hours\n",
      "|\\n| Image Acquisition                   | December 28, 2018 by Landsat 8 OLI\n",
      "|\\n| Location of Eruption                | Southeastern side of Mount Etna\n",
      "|\\n| Thermal Imaging Data                | From OLI and TIRS (Landsat 8), highlighting active vent and lava flows\n",
      "|\\n| Impact on Villages/Air Transport    | Ash covered villages; delayed aircraft at Catania airport\n",
      "|\\n| Displacement                        | Hundreds of residents displaced\n",
      "|\\n| Ongoing Seismic Activity            | Earthquakes continued after initial eruption\n",
      "|\\n\\n---\\n\\n<!-- PageFooter=\\\"Earth at Night\\\" -->\\n<!-- PageNumber=\\\"30\\\" -->\"},{\"ref_id\":2,\"content\":\"<!--\n",
      "PageHeader=\\\"Volcanoes\\\" -->\\n\\n### Nighttime Glow at Mount Etna - Italy\\n\\nAt about 2:30 a.m. local time on March 16,\n",
      "2017, the VIIRS DNB on the Suomi NPP satellite captured this nighttime image of lava flowing on Mount Etna in Sicily,\n",
      "Italy. Etna is one of the world's most active volcanoes.\\n\\n#### Figure: Location of Mount Etna\\nA world globe is\n",
      "depicted, with a marker indicating the location of Mount Etna in Sicily, Italy, in southern Europe near the center of\n",
      "the Mediterranean Sea.\\n\\n<!-- PageFooter=\\\"Earth at Night\\\" -->\\n<!-- PageNumber=\\\"48\\\" -->\"},{\"ref_id\":3,\"content\":\"##\n",
      "Nature's Light Shows\\n\\nAt night, with the light of the Sun removed, nature's brilliant glow from Earth's surface\n",
      "becomes visible to the naked eye from space. Some of Earth's most spectacular light shows are natural, like the aurora\n",
      "borealis, or Northern Lights, in the Northern Hemisphere (aurora australis, or Southern Lights, in the Southern\n",
      "Hemisphere). The auroras are natural electrical phenomena caused by charged particles that race from the Sun toward\n",
      "Earth, inducing chemical reactions in the upper atmosphere and creating the appearance of streamers of reddish or\n",
      "greenish light in the sky, usually near the northern or southern magnetic pole. Other natural lights can indicate\n",
      "danger, like a raging forest fire encroaching on a city, town, or community, or lava spewing from an erupting\n",
      "volcano.\\n\\nWhatever the source, the ability of humans to monitor nature's light shows at night has practical\n",
      "applications for society. For example, tracking fires during nighttime hours allows for continuous monitoring and\n",
      "enhances our ability to protect humans and other animals, plants, and infrastructure. Combined with other data sources,\n",
      "our ability to observe the light of fires at night allows emergency managers to more efficiently and accurately issue\n",
      "warnings and evacuation orders and allows firefighting efforts to continue through the night. With enough moonlight\n",
      "(e.g., full-Moon phase), it's even possible to track the movement of smoke plumes at night, which can impact air\n",
      "quality, regardless of time of day.\\n\\nAnother natural source of light at night is emitted from glowing lava flows at\n",
      "the site of active volcanoes. Again, with enough moonlight, these dramatic scenes can be tracked and monitored for both\n",
      "scientific research and public safety.\\n\\n\\n### Figure: The Northern Lights Viewed from Space\\n\\n**September 17,\n",
      "2011**\\n\\nThis photo, taken from the International Space Station on September 17, 2011, shows a spectacular display of\n",
      "the aurora borealis (Northern Lights) as green and reddish light in the night sky above Earth. In the foreground, part\n",
      "of a Soyuz spacecraft is visible, silhouetted against the bright auroral light. The green glow is generated by energetic\n",
      "charged particles from the Sun interacting with Earth's upper atmosphere, exciting oxygen and nitrogen atoms, and\n",
      "producing characteristic colors. The image demonstrates the vividness and grandeur of natural night-time light phenomena\n",
      "as seen from orbit.\"}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Response\")\n",
    "print(textwrap.fill(retrieval_result.response[0].content[0].text, width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f74c2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity\n",
      "[\n",
      "  {\n",
      "    \"id\": 0,\n",
      "    \"type\": \"ModelQueryPlanning\",\n",
      "    \"input_tokens\": 2739,\n",
      "    \"output_tokens\": 122\n",
      "  },\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"type\": \"AzureSearchQuery\",\n",
      "    \"target_index\": \"earth_at_night\",\n",
      "    \"query\": {\n",
      "      \"search\": \"Methods to locate lava flows at night\"\n",
      "    },\n",
      "    \"query_time\": \"2025-06-28T10:17:29.948Z\",\n",
      "    \"count\": 4,\n",
      "    \"elapsed_ms\": 372\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"type\": \"AzureSearchSemanticRanker\",\n",
      "    \"input_tokens\": 24436\n",
      "  }\n",
      "]\n",
      "Results\n",
      "[\n",
      "  {\n",
      "    \"type\": \"AzureSearchDoc\",\n",
      "    \"id\": \"0\",\n",
      "    \"activity_source\": 1,\n",
      "    \"doc_key\": \"earth_at_night_508_page_60_verbalized\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"AzureSearchDoc\",\n",
      "    \"id\": \"1\",\n",
      "    \"activity_source\": 1,\n",
      "    \"doc_key\": \"earth_at_night_508_page_46_verbalized\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"AzureSearchDoc\",\n",
      "    \"id\": \"2\",\n",
      "    \"activity_source\": 1,\n",
      "    \"doc_key\": \"earth_at_night_508_page_64_verbalized\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"AzureSearchDoc\",\n",
      "    \"id\": \"3\",\n",
      "    \"activity_source\": 1,\n",
      "    \"doc_key\": \"earth_at_night_508_page_44_verbalized\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(\"Activity\")\n",
    "print(json.dumps([a.as_dict() for a in retrieval_result.activity], indent=2))\n",
    "print(\"Results\")\n",
    "print(json.dumps([r.as_dict() for r in retrieval_result.references], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d32cc",
   "metadata": {},
   "source": [
    "## Generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6486c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find lava at night, you can use thermal imaging from satellites, which detect the heat emitted by\n",
      "lava flows. Instruments like the VIIRS (Visible Infrared Imaging Radiometer Suite) Day/Night Band\n",
      "can pick up the infrared glow even in low-light conditions. This allows for the observation of\n",
      "active lava flows and volcanic eruptions, providing key data for tracking and monitoring these\n",
      "natural phenomena for safety and research purposes [1][2][3].\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=answer_model,\n",
    "    input=messages\n",
    ")\n",
    "\n",
    "wrapped = textwrap.fill(response.output_text, width=100)\n",
    "print(wrapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75777ed2",
   "metadata": {},
   "source": [
    "## Clean up objects and resources\n",
    "\n",
    "If you no longer need Azure AI Search or Azure OpenAI, delete them from your Azure subscription. You can also start over by deleting individual objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f6fe6",
   "metadata": {},
   "source": [
    "### Delete the knowledge agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67b6a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge agent 'earth-search-agent' deleted successfully\n"
     ]
    }
   ],
   "source": [
    "index_client = SearchIndexClient(endpoint=endpoint, credential=credential)\n",
    "index_client.delete_agent(agent_name)\n",
    "print(f\"Knowledge agent '{agent_name}' deleted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35bfbb1",
   "metadata": {},
   "source": [
    "### Delete the search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25f5e6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'earth_at_night' deleted successfully\n"
     ]
    }
   ],
   "source": [
    "index_client = SearchIndexClient(endpoint=endpoint, credential=credential)\n",
    "index_client.delete_index(index)\n",
    "print(f\"Index '{index_name}' deleted successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
